# 2020-09-18 Meeting

## 1. Two Stage InfoMax-VAE

Fix the bug of the oscillating loss score of mutual information estimations   

**Hyperparameters**:

Hyperparameters: m=7387, Epoch=150, input=(64, 64, 3), alpha=1, beta =1, gamma=0.8

**The KL Divergence:**

Approximation in VAE shown in [derivation](https://arxiv.org/pdf/1907.08956.pdf): 
$$
\begin{align*}
KL(q_\theta(z|x_i) || p(z)) &= -\sum_z\frac{1}{2}[1 + \log(\sigma_q^2) -\sigma_q^2 - \mu_q^2 ]
\end{align*}
$$
The difference cannot be evaluated directly from the loss graph. Since they have different terms. 

#### **Ver. 1** Single Embed

<img src="single_embed/single_embed.png" alt="not_double_embeded" style="zoom:40%;" />

Note that we have to still consider the KL loss of reconstruction between $X_2$ and $X$ to update the weight of ***Decoder 2*** properly.
$$
L(\theta_1, \phi_1, \theta_2, \phi_2) = (ReconLoss(x, x_2) - \beta_2 * KL + \alpha_2 * MI(x, z_2)) + \gamma * (ReconLoss(x, x_1) \\ - \beta_1 * KL + \alpha_1 * MI(x, z_1))
$$

##### Training and Results

<img src="single_embed/los_evolution.png" style="zoom:70%;"/>

<img src="single_embed/reconstructions.png" style="zoom:100%;"/>

#### **Ver. 2 Double Embed**

<img src="double_embed/double_embed.png" alt="not_double_embeded" style="zoom:40%;" />
$$
L = (ReconLoss(x, x_2) - \beta_2 * KL + \alpha_2 * MI(x, z_2)) + \gamma * (ReconLoss(x, x_1) \\ - \beta_1 * KL + \alpha_1 * MI(x, z_1))
$$

##### Training and Results

<img src="double_embed/los_evolution.png" style="zoom:80%;"/>

<img src="double_embed/reconstructions.png" style="zoom:100%;"/>The VAE1 has fitted better to the prior. Since it measures the MI between z1 and z2, it is expected to see a very close curves. 

#### Results

<img src="vs_2_stage_vae.png" alt="not_double_embeded" style="zoom:50%;" />

**TODO:** Annotate this 

dsn't mean loss of information, nice to show, however, cannot draw important conclusion from this 

need strong support from quantitative metrics. 

#### **TODOs**

> - Apply a beta larger than 1 ($\beta$-VAE)
> - Apply different value of $\gamma$ for $MI$
>
> - Find out how MINE is approximated

## 2. Inference and Explanation 

Since the prior of the latent space of VAE is Gaussian.

<img src="dist.png" style="zoom:30%;" />

We permutate uniformly (n=15) across the data distribution (95%) of the 3 latent feature ($i, j, k$). With each step $\delta$ approach from one end to another. However, we can only model 2 features on a graph at a time.

#### **Individual channels**

<img src="channels.png" alt="not_double_embeded" style="zoom:100%;" />

In the graph shown above, each axis represent a feature and each 

#### RGB

<img src="rgb.png" alt="not_double_embeded" style="zoom:30%;" />

As shown below, the z3 dimension of the latent feature has contributed to the green and blue colour of cell images.

<img src="series.png" alt="not_double_embeded" style="zoom:50%;" />

Incorporate with the grand truth label (class=1, 2, 3, 4) and **conditional-VAE**. We can exaggerate the effect of each perturbation. 

## Plan 

- More insight with perturbation.
- Compare ***Explanation by progressive exaggeration ICLR 2020*** with 77 pages paper 
- Hyperparameter Tuning with $\alpha, \beta, \gamma$. 

## Meeting Notes:

- is it possible to assess, is that recon() for each latent space model strong or weak, individual divergence per input point, quality of the fit 

- The comparison of generated images dsn't mean loss of information, nice to show, however, cannot draw important conclusion from this 

  need strong support from quantitative metrics. 

- Check local **neighbourhood preservation** (from Sacha's paper), which part of the manifold, map diff GT and 

- The channel from BBBC has RGB channels, try another datasets, 

